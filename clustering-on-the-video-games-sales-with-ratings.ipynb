{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom kmodes.kprototypes import KPrototypes\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T00:37:55.83154Z","iopub.execute_input":"2022-04-13T00:37:55.832024Z","iopub.status.idle":"2022-04-13T00:37:55.842136Z","shell.execute_reply.started":"2022-04-13T00:37:55.831989Z","shell.execute_reply":"2022-04-13T00:37:55.841464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{}},{"cell_type":"code","source":"vg_df = pd.read_csv('../input/video-game-sales-with-ratings/Video_Games_Sales_as_at_22_Dec_2016.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.159221Z","iopub.execute_input":"2022-04-13T00:28:47.159527Z","iopub.status.idle":"2022-04-13T00:28:47.234582Z","shell.execute_reply.started":"2022-04-13T00:28:47.159487Z","shell.execute_reply":"2022-04-13T00:28:47.233959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking in to the data\nvg_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.235783Z","iopub.execute_input":"2022-04-13T00:28:47.236252Z","iopub.status.idle":"2022-04-13T00:28:47.266177Z","shell.execute_reply.started":"2022-04-13T00:28:47.236195Z","shell.execute_reply":"2022-04-13T00:28:47.265309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the dataframe info\nvg_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.268165Z","iopub.execute_input":"2022-04-13T00:28:47.269038Z","iopub.status.idle":"2022-04-13T00:28:47.29951Z","shell.execute_reply.started":"2022-04-13T00:28:47.268995Z","shell.execute_reply":"2022-04-13T00:28:47.298829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be some missing data. Let's explore that","metadata":{}},{"cell_type":"markdown","source":"# Missing Data","metadata":{}},{"cell_type":"code","source":"# Calculate the missing values percentage in each column\nmissing_total = vg_df.isnull().sum().sort_values(ascending=False)\npercent = ((missing_total*100)/(vg_df.isnull().count())).sort_values(ascending=False)\npercent","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.300612Z","iopub.execute_input":"2022-04-13T00:28:47.301062Z","iopub.status.idle":"2022-04-13T00:28:47.329019Z","shell.execute_reply.started":"2022-04-13T00:28:47.301021Z","shell.execute_reply":"2022-04-13T00:28:47.328426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our primary goal is visualisation, we will drop the rows wherever there is some missing data.","metadata":{}},{"cell_type":"markdown","source":"# Dropping the rows with NA","metadata":{}},{"cell_type":"code","source":"vg_df.dropna(axis=0, inplace=True)\nvg_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.330009Z","iopub.execute_input":"2022-04-13T00:28:47.330331Z","iopub.status.idle":"2022-04-13T00:28:47.355408Z","shell.execute_reply.started":"2022-04-13T00:28:47.330304Z","shell.execute_reply":"2022-04-13T00:28:47.354487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, all the columns are having data and no missing values are present.","metadata":{"execution":{"iopub.status.busy":"2022-04-12T22:37:53.347308Z","iopub.execute_input":"2022-04-12T22:37:53.348451Z","iopub.status.idle":"2022-04-12T22:37:53.354524Z","shell.execute_reply.started":"2022-04-12T22:37:53.348407Z","shell.execute_reply":"2022-04-12T22:37:53.3533Z"}}},{"cell_type":"code","source":"vg_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.356662Z","iopub.execute_input":"2022-04-13T00:28:47.35719Z","iopub.status.idle":"2022-04-13T00:28:47.378281Z","shell.execute_reply.started":"2022-04-13T00:28:47.357144Z","shell.execute_reply":"2022-04-13T00:28:47.377426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking the datatypes of the columns\nvg_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.379565Z","iopub.execute_input":"2022-04-13T00:28:47.380477Z","iopub.status.idle":"2022-04-13T00:28:47.397301Z","shell.execute_reply.started":"2022-04-13T00:28:47.380432Z","shell.execute_reply":"2022-04-13T00:28:47.396421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the year of relase is a date column and not a continuous column, we will convert it in to a object and pass to the clustering algorithm.\n\nThe User_Score column should also be converted into float from objectg.","metadata":{}},{"cell_type":"code","source":"# Converting the datatype of year_of_relase and User_score columns\nvg_df['Year_of_Release'] = vg_df['Year_of_Release'].astype(str)\nvg_df['User_Score'] = vg_df['User_Score'].astype(float)\nvg_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.39867Z","iopub.execute_input":"2022-04-13T00:28:47.398998Z","iopub.status.idle":"2022-04-13T00:28:47.421432Z","shell.execute_reply.started":"2022-04-13T00:28:47.39897Z","shell.execute_reply":"2022-04-13T00:28:47.420867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vg_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.423597Z","iopub.execute_input":"2022-04-13T00:28:47.423962Z","iopub.status.idle":"2022-04-13T00:28:47.444797Z","shell.execute_reply.started":"2022-04-13T00:28:47.42393Z","shell.execute_reply":"2022-04-13T00:28:47.444014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the no.of unique values in the categorical columns\nvg_df.select_dtypes('object').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.446066Z","iopub.execute_input":"2022-04-13T00:28:47.446269Z","iopub.status.idle":"2022-04-13T00:28:47.460931Z","shell.execute_reply.started":"2022-04-13T00:28:47.446246Z","shell.execute_reply":"2022-04-13T00:28:47.460259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have high number of unique values in the Name, Developer and Publisher columns we will drop as they are no good for clustering.","metadata":{}},{"cell_type":"code","source":"# Dropping the unnecesasry columns for clustering.\nvg_df1 = vg_df.drop(['Name','Developer', 'Publisher'], axis=1)\nvg_df1.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.462196Z","iopub.execute_input":"2022-04-13T00:28:47.462405Z","iopub.status.idle":"2022-04-13T00:28:47.482036Z","shell.execute_reply.started":"2022-04-13T00:28:47.462381Z","shell.execute_reply":"2022-04-13T00:28:47.481392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Before we scale the data we must reset the index, so that we can this index to join back the scaled numerical columned dataframe with the original datafram with categorical columns","metadata":{}},{"cell_type":"code","source":"vg_df1.reset_index(inplace=True, drop=True)\nvg_df1.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.483279Z","iopub.execute_input":"2022-04-13T00:28:47.483576Z","iopub.status.idle":"2022-04-13T00:28:47.506774Z","shell.execute_reply.started":"2022-04-13T00:28:47.483547Z","shell.execute_reply":"2022-04-13T00:28:47.506149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting the categorical columns as a separate dataframe so that we can use this to join with scaled numerical columns later","metadata":{"execution":{"iopub.status.busy":"2022-04-12T23:32:05.7683Z","iopub.execute_input":"2022-04-12T23:32:05.76902Z","iopub.status.idle":"2022-04-12T23:32:05.773588Z","shell.execute_reply.started":"2022-04-12T23:32:05.768951Z","shell.execute_reply":"2022-04-12T23:32:05.772463Z"}}},{"cell_type":"code","source":"vg_df_cat = vg_df1[['Platform', 'Year_of_Release', 'Genre', 'Rating']]\nvg_df_cat.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.507742Z","iopub.execute_input":"2022-04-13T00:28:47.508075Z","iopub.status.idle":"2022-04-13T00:28:47.519294Z","shell.execute_reply.started":"2022-04-13T00:28:47.508049Z","shell.execute_reply":"2022-04-13T00:28:47.518617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's select the numerical columns and scale them","metadata":{}},{"cell_type":"markdown","source":"# Scaling the Data","metadata":{}},{"cell_type":"markdown","source":"As we have the numerical columns ranging the data in differnt ranges, we need to scale them. For this purpose we will use the MinMaxScaler()","metadata":{}},{"cell_type":"code","source":"# Selecting the numerical columns for scaling\nvg_df_num = vg_df1[['NA_Sales', 'EU_Sales', 'JP_Sales','Other_Sales','Global_Sales','Critic_Score','Critic_Count','User_Score','User_Count']]\nvg_df_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.520458Z","iopub.execute_input":"2022-04-13T00:28:47.520834Z","iopub.status.idle":"2022-04-13T00:28:47.545412Z","shell.execute_reply.started":"2022-04-13T00:28:47.520793Z","shell.execute_reply":"2022-04-13T00:28:47.544828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracting the column names of the dataframe vg_df_num","metadata":{"execution":{"iopub.status.busy":"2022-04-12T23:22:32.967135Z","iopub.execute_input":"2022-04-12T23:22:32.96813Z","iopub.status.idle":"2022-04-12T23:22:32.974203Z","shell.execute_reply.started":"2022-04-12T23:22:32.968032Z","shell.execute_reply":"2022-04-12T23:22:32.973086Z"}}},{"cell_type":"markdown","source":"Next we have to scale the data in vg_df_num. But while doing that we will get the vg_df_num as numpy array. So, in order to convert again it to a dataframe we will first extract the column names of the dataframe vg_df_num before scaling.","metadata":{}},{"cell_type":"code","source":"vg_df_num_columns = list(vg_df_num.columns)\nprint(vg_df_num_columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.546271Z","iopub.execute_input":"2022-04-13T00:28:47.546978Z","iopub.status.idle":"2022-04-13T00:28:47.551782Z","shell.execute_reply.started":"2022-04-13T00:28:47.546946Z","shell.execute_reply":"2022-04-13T00:28:47.550888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mix-max scale the data between 0 and 1\nmms = MinMaxScaler()\nvg_df_num = mms.fit_transform(vg_df_num)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.552877Z","iopub.execute_input":"2022-04-13T00:28:47.553786Z","iopub.status.idle":"2022-04-13T00:28:47.566395Z","shell.execute_reply.started":"2022-04-13T00:28:47.553728Z","shell.execute_reply":"2022-04-13T00:28:47.565351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vg_df_num","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.567637Z","iopub.execute_input":"2022-04-13T00:28:47.568511Z","iopub.status.idle":"2022-04-13T00:28:47.576897Z","shell.execute_reply.started":"2022-04-13T00:28:47.568469Z","shell.execute_reply":"2022-04-13T00:28:47.575954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting vg_df_num back to dataframe","metadata":{"execution":{"iopub.status.busy":"2022-04-12T23:19:09.175543Z","iopub.execute_input":"2022-04-12T23:19:09.17641Z","iopub.status.idle":"2022-04-12T23:19:09.181215Z","shell.execute_reply.started":"2022-04-12T23:19:09.176348Z","shell.execute_reply":"2022-04-12T23:19:09.180241Z"}}},{"cell_type":"markdown","source":"As seen in the above output vg_df_num is now converted in to a numpy array. So, we will convert it back to the Dataframe back, so that we can visualize the data in some columns before and after scaling.","metadata":{}},{"cell_type":"code","source":"# converting the X to dataframe\nvg_df_num = pd.DataFrame(vg_df_num)\nvg_df_num.columns = vg_df_num_columns\nvg_df_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.578018Z","iopub.execute_input":"2022-04-13T00:28:47.578361Z","iopub.status.idle":"2022-04-13T00:28:47.598038Z","shell.execute_reply.started":"2022-04-13T00:28:47.578326Z","shell.execute_reply":"2022-04-13T00:28:47.597418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets join the data dataframes vg_df_num and vg_df_cat","metadata":{}},{"cell_type":"code","source":"# Joining the scaled dataframe and categorical dataframe\nvg_df_scaled = pd.concat([vg_df_cat, vg_df_num], axis=1)\nvg_df_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.599024Z","iopub.execute_input":"2022-04-13T00:28:47.599335Z","iopub.status.idle":"2022-04-13T00:28:47.620602Z","shell.execute_reply.started":"2022-04-13T00:28:47.599308Z","shell.execute_reply":"2022-04-13T00:28:47.619652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering","metadata":{"execution":{"iopub.status.busy":"2022-04-12T23:37:58.199912Z","iopub.execute_input":"2022-04-12T23:37:58.200246Z","iopub.status.idle":"2022-04-12T23:37:58.205534Z","shell.execute_reply.started":"2022-04-12T23:37:58.20021Z","shell.execute_reply":"2022-04-12T23:37:58.204231Z"}}},{"cell_type":"markdown","source":"Let's try to cluster the data and find out the similar featured observations that can fall in to similar group.","metadata":{}},{"cell_type":"markdown","source":"As we have categorical and numerical variables for clustering, we will use the KPrototypes clustering.","metadata":{}},{"cell_type":"markdown","source":"As the kprototypes clustering algorithm needs the explicit positions of the categorical columns, lets try to find them.","metadata":{"execution":{"iopub.status.busy":"2022-04-12T23:45:08.378882Z","iopub.execute_input":"2022-04-12T23:45:08.37921Z","iopub.status.idle":"2022-04-12T23:45:08.386436Z","shell.execute_reply.started":"2022-04-12T23:45:08.379175Z","shell.execute_reply":"2022-04-12T23:45:08.385261Z"}}},{"cell_type":"code","source":"# Get the position of categorical columns\ncatColumnsPos = [vg_df_scaled.columns.get_loc(col) for col in list(vg_df_scaled.select_dtypes('object').columns)]\nprint('Categorical columns           : {}'.format(list(vg_df_scaled.select_dtypes('object').columns)))\nprint('Categorical columns position  : {}'.format(catColumnsPos))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.621859Z","iopub.execute_input":"2022-04-13T00:28:47.62216Z","iopub.status.idle":"2022-04-13T00:28:47.630896Z","shell.execute_reply.started":"2022-04-13T00:28:47.62213Z","shell.execute_reply":"2022-04-13T00:28:47.630225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the dataframe in to a numpy array\nvg_df_array = vg_df_scaled.values\nvg_df_array","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.631797Z","iopub.execute_input":"2022-04-13T00:28:47.63229Z","iopub.status.idle":"2022-04-13T00:28:47.650293Z","shell.execute_reply.started":"2022-04-13T00:28:47.632261Z","shell.execute_reply":"2022-04-13T00:28:47.649434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### explicitly mentioning the numerical columns by converting them to float datatype","metadata":{}},{"cell_type":"code","source":"vg_df_array[:, 4:13] = vg_df_array[:, 4:13].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.651201Z","iopub.execute_input":"2022-04-13T00:28:47.651989Z","iopub.status.idle":"2022-04-13T00:28:47.657621Z","shell.execute_reply.started":"2022-04-13T00:28:47.651946Z","shell.execute_reply":"2022-04-13T00:28:47.656735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vg_df_array","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:28:47.659077Z","iopub.execute_input":"2022-04-13T00:28:47.659526Z","iopub.status.idle":"2022-04-13T00:28:47.670299Z","shell.execute_reply.started":"2022-04-13T00:28:47.659424Z","shell.execute_reply":"2022-04-13T00:28:47.669678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# kprototypes clustering implementation","metadata":{}},{"cell_type":"markdown","source":"# Finding the Optimal number of clusters","metadata":{}},{"cell_type":"code","source":"# Trying the range of clusters from 1 to 10\ncost = []\nfor cluster in range(1, 10):\n    kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 0)\n    kprototype.fit_predict(vg_df_array, categorical = catColumnsPos)\n    cost.append(kprototype.cost_)\n    print('Cluster initiation: {}'.format(cluster))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:38:22.27764Z","iopub.execute_input":"2022-04-13T00:38:22.278421Z","iopub.status.idle":"2022-04-13T00:47:51.941135Z","shell.execute_reply.started":"2022-04-13T00:38:22.278373Z","shell.execute_reply":"2022-04-13T00:47:51.939393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the results into a dataframe and plotting them\ndf_cost = pd.DataFrame({'Cluster':range(1, 10), 'Cost':cost})\ndf_cost","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:48:06.584378Z","iopub.execute_input":"2022-04-13T00:48:06.584669Z","iopub.status.idle":"2022-04-13T00:48:06.599255Z","shell.execute_reply.started":"2022-04-13T00:48:06.584631Z","shell.execute_reply":"2022-04-13T00:48:06.598703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the clusters to cost function.\nsns.lineplot(x='Cluster', y='Cost', data=df_cost)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:48:11.094259Z","iopub.execute_input":"2022-04-13T00:48:11.094793Z","iopub.status.idle":"2022-04-13T00:48:11.319925Z","shell.execute_reply.started":"2022-04-13T00:48:11.09473Z","shell.execute_reply":"2022-04-13T00:48:11.319259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above plot, by using the elbow method we can clearly see an elbow at 3 clusters. The change for every unit increase in the clusters number in the cost is not substantial. So, the optimal number of clusters is going to be 3. ","metadata":{}},{"cell_type":"code","source":"# Implementing kprototype algorithm\nkproto = KPrototypes(n_clusters=3, verbose=2, max_iter=20)\nclusters = kproto.fit_predict(vg_df_array, categorical=[0,1,2,3])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:49:39.621154Z","iopub.execute_input":"2022-04-13T00:49:39.621432Z","iopub.status.idle":"2022-04-13T00:50:39.691698Z","shell.execute_reply.started":"2022-04-13T00:49:39.621399Z","shell.execute_reply":"2022-04-13T00:50:39.691007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the cluster centroids\nprint(kproto.cluster_centroids_)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:12.331601Z","iopub.execute_input":"2022-04-13T00:52:12.33192Z","iopub.status.idle":"2022-04-13T00:52:12.337976Z","shell.execute_reply.started":"2022-04-13T00:52:12.331887Z","shell.execute_reply":"2022-04-13T00:52:12.337115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing all the clustered numbers\ncluster_dict=[]\nfor c in clusters:\n    cluster_dict.append(c)\nprint(cluster_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:15.150768Z","iopub.execute_input":"2022-04-13T00:52:15.151414Z","iopub.status.idle":"2022-04-13T00:52:15.159884Z","shell.execute_reply.started":"2022-04-13T00:52:15.151379Z","shell.execute_reply":"2022-04-13T00:52:15.159031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attach the clustering output to the original dataframe vg_df_scaled\nvg_df_scaled['cluster']=cluster_dict\nvg = vg_df_scaled\nvg.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:19.273055Z","iopub.execute_input":"2022-04-13T00:52:19.273594Z","iopub.status.idle":"2022-04-13T00:52:19.294512Z","shell.execute_reply.started":"2022-04-13T00:52:19.273558Z","shell.execute_reply":"2022-04-13T00:52:19.293702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attach the clustering output to the original dataframe vg_df\nvg_df['cluster']=cluster_dict\nvg = vg_df\nvg.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:33.551479Z","iopub.execute_input":"2022-04-13T00:52:33.552251Z","iopub.status.idle":"2022-04-13T00:52:33.576059Z","shell.execute_reply.started":"2022-04-13T00:52:33.552212Z","shell.execute_reply":"2022-04-13T00:52:33.57524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the cluster-0\nvg[vg['cluster']==0].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:45.824181Z","iopub.execute_input":"2022-04-13T00:52:45.824451Z","iopub.status.idle":"2022-04-13T00:52:45.849892Z","shell.execute_reply.started":"2022-04-13T00:52:45.824425Z","shell.execute_reply":"2022-04-13T00:52:45.849297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the cluster-1\nvg[vg['cluster']==1].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:50.410276Z","iopub.execute_input":"2022-04-13T00:52:50.410867Z","iopub.status.idle":"2022-04-13T00:52:50.434047Z","shell.execute_reply.started":"2022-04-13T00:52:50.410826Z","shell.execute_reply":"2022-04-13T00:52:50.433271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the cluster-0\nvg[vg['cluster']==2].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:52:53.826351Z","iopub.execute_input":"2022-04-13T00:52:53.826602Z","iopub.status.idle":"2022-04-13T00:52:53.849052Z","shell.execute_reply.started":"2022-04-13T00:52:53.826573Z","shell.execute_reply":"2022-04-13T00:52:53.848413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exporting the dataframe as a CSV","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:54:19.532879Z","iopub.execute_input":"2022-04-13T00:54:19.533193Z","iopub.status.idle":"2022-04-13T00:54:19.537041Z","shell.execute_reply.started":"2022-04-13T00:54:19.533163Z","shell.execute_reply":"2022-04-13T00:54:19.536181Z"}}},{"cell_type":"markdown","source":"Let's export the cleaned dataframe to a csv file","metadata":{}},{"cell_type":"code","source":"vg.to_csv('videogames_sales_cleaned.CSV')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T00:54:52.85612Z","iopub.execute_input":"2022-04-13T00:54:52.856393Z","iopub.status.idle":"2022-04-13T00:54:52.922368Z","shell.execute_reply.started":"2022-04-13T00:54:52.856365Z","shell.execute_reply":"2022-04-13T00:54:52.921764Z"},"trusted":true},"execution_count":null,"outputs":[]}]}